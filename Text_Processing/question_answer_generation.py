# -*- coding: utf-8 -*-
"""
Created on Fri Sep  6 08:52:48 2024

@author: Enrico
"""
import openai
from langchain.text_splitter import RecursiveCharacterTextSplitter
#from langchain.document_loaders import TextLoader
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import re
import sys
import pandas as pd
import os

##### Helper Functions  ########################################################

#Sliding window can be implemented by taking a fixed-size window of text with some overlap
# NOt USED !
# def sliding_window(text, window_size, overlap):
#     words = text.split()
#     chunks = []
#     for i in range(0, len(words), window_size - overlap):
#         chunk = " ".join(words[i:i + window_size])
#         chunks.append(chunk)
#     return chunks

def clean_text(text):
    """
     Cleans the input text by removing metadata, URLs, page numbers, and
     other unwanted content. Extracts the relevant content starting from 'CHAPTER ONE'.
     """
    # Remove lines related to the metadata, like "Generated by ABC Amber LIT Converter"
    cleaned_text = re.sub(r"Generated by ABC Amber LIT Converter.*?\n", "",text)
    # Remove URLs from the text
    cleaned_text = re.sub(r"http[s]?://\S+|www\.\S+", "", cleaned_text)
    # Remove any URLs or other extraneous content
    #cleaned_text = re.sub(r"http\S+", "", cleaned_text)
    # Remove empty lines or excessive whitespace
    cleaned_text = re.sub(r"\n\s*\n", "\n", cleaned_text)
    # Remove page numbers and unwanted parts
    cleaned_text = re.sub(r'Page \d+', '', cleaned_text)  # Remove page numbers
    cleaned_text = re.sub(r'\bCHAPTER\b.*', '', cleaned_text)  # Remove chapter titles 
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Remove excess whitespace
    start_index = cleaned_text.find("CHAPTER ONE")

    # Extract only the relevant content starting from the first chapter
    if start_index != -1:
        cleaned_text = cleaned_text[start_index:]
    else:
        cleaned_text = cleaned_text  # If not found, assume the whole text is relevant
        
    #cleaned_text = re.sub(r"harry potter\s*&\s*the sorcerer’s stone\s*by\s*j\.k\. rowling", "", cleaned_text)

    # Remove chapter headings (optional)
    #cleaned_text = re.sub(r"chapter\s*\w+\s*\n", "", cleaned_text)

    return cleaned_text.strip()

# Function to save checkpoint
def save_checkpoint(questions, answers, chunks_done,checkpoint_file):
    """Saves the current questions, answers, and chunk progress to a checkpoint file."""
    df_checkpoint = pd.DataFrame({
        'Question': questions,
        'Answer': answers,
        'Chunk_Index': chunks_done
    })
    df_checkpoint.to_csv(checkpoint_file, index=False)

# Function to load checkpoint if it exists
def load_checkpoint(checkpoint_file):
    """Loads the checkpoint file if it exists and returns questions, answers, and chunks processed."""
    if os.path.exists(checkpoint_file):
        df_checkpoint = pd.read_csv(checkpoint_file)
        questions = df_checkpoint['Question'].tolist()
        answers = df_checkpoint['Answer'].tolist()
        chunks_done = df_checkpoint['Chunk_Index'].tolist()
        return questions, answers, chunks_done
    return [], [], []

###############################################################################

# Main Process
if __name__ == "__main__":
    
    # Set up your OpenAI API key
    openai.api_key = ' Your OpenAI API key '
    ### txt path
    path_txt = 'Your text file'
    # Checkpoint file path
    checkpoint_file = './questions_and_answers_checkpoint.csv'
    # final df file path
    path_final_file = './questions_and_answers_final.csv'
    # Set up the OpenAI LLM (make sure to set your OpenAI API key)
    llm = OpenAI(temperature=0.7, api_key=openai.api_key)
    
    ###############################################################################
    # Test LLM connection
    try:
        # Example prompt
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is the capital of France?"}
            ],
            max_tokens=30  # Reduced to 30 to lower the token usage
        )
    
        # Print the output
        print(response.choices[0].message.content.strip())
        print("LLM Connection Successful!")
    except Exception as e:
        print("Failed to connect to OpenAI:", e)
        sys.exit()

    ##### Load and Clean Text #################################################
    with open(path_txt, "r",  encoding="utf-8", errors="ignore") as file:
        text_raw = file.read()
        
    ### Step 1  Clean the text content
    text_content = clean_text(text_raw)
    
    ### Step 2 Wrap the content in LangChain's document format
    documents = [Document(page_content=text_content)] ## 
    
    # Step 3: Text Chunking #ù
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=20
    )
    chunks = text_splitter.split_documents(documents)
    
    # Step 4: Define Prompts for Question and Answer Generation
    question_prompt_template = PromptTemplate.from_template(
        "Generate a question based on the following text:\n\n{chunk}\n\nQuestion:"
    )
    
    answer_prompt_template = PromptTemplate.from_template(
        "Provide an answer based on the following text:\n\n{chunk}\n\nAnswer:"
    )
    
    # Generate questions for each chunk using llm #################################
    # Initialize empty lists to store questions and answers
    questions = []
    answers = []
    chunks_done = []
    # Initialize questions and answers lists from the checkpoint, if it exists
    questions, answers, chunks_done = load_checkpoint(checkpoint_file)
    # Resume from the last saved chunk
    start_chunk = max(chunks_done) + 1 if chunks_done else 0
    # Get the total number of chunks
    total_chunks = len(chunks)
    # Set the number of questions you want to generate per chunk
    num_iterations_per_chunk = 20
    # Save checkpoint every N chunks (e.g., every 10 chunks)
    save_checkpoint_every_n_chunks = 10
    # Iterate through each chunk
    for i, chunk in enumerate(chunks[start_chunk:], start=start_chunk):
        chunk_text = chunk.page_content  # Get the content of the chunk
        print(f"Chunk {i+1}/{total_chunks}")
        for j in range(num_iterations_per_chunk):
            # Generate a question from the chunk
            question_prompt = question_prompt_template.format(chunk=chunk_text)
            question = llm.invoke(question_prompt)
        
            # Generate an answer from the chunk
            answer_prompt = answer_prompt_template.format(chunk=chunk_text)
            answer = llm.invoke(answer_prompt)
        
            # Append the question and answer to the lists
            questions.append(question)
            answers.append(answer)
    
            print(f"Chunk {i+1} - Iteration {j+1}/{num_iterations_per_chunk}")
            # print(f"Question: {question}")
            # print(f"Answer: {answer}")
            # print("\n---\n")
            # Add the completed chunk index
            chunks_done.append(i)
        # Save the checkpoint every N chunks
        if (i + 1) % save_checkpoint_every_n_chunks == 0:
             save_checkpoint(questions, answers, chunks_done,checkpoint_file)
             print(f"Checkpoint saved after chunk {i+1}.")
    
    
    ##### Save Final Questions and Answers to CSV #############################
    df_qa = pd.DataFrame({
        'Question': questions,
        'Answer': answers
    })
    
    df_qa.to_csv(path_final_file, index=False)
    print(f"Final qa csv saved .")
