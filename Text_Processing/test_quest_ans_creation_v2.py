# -*- coding: utf-8 -*-
"""
Created on Fri Sep  6 08:52:48 2024

@author: Enrico
"""

###processing vreating question answer ########################################
import openai
from langchain.text_splitter import RecursiveCharacterTextSplitter
#from langchain.document_loaders import TextLoader
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import re
import sys
import pandas as pd
import os
import random
##### some useful def ########################################################
#Sliding window can be implemented by taking a fixed-size window of text with some overlap
# NOt USED !
def sliding_window(text, window_size, overlap):
    words = text.split()
    chunks = []
    for i in range(0, len(words), window_size - overlap):
        chunk = " ".join(words[i:i + window_size])
        chunks.append(chunk)
    return chunks
###############################################################################
def clean_text(text):
    # Remove lines related to the metadata
    cleaned_text = re.sub(r"Generated by ABC Amber LIT Converter.*?\n", "", text)
    
    # Remove URLs from the text
    cleaned_text = re.sub(r"http[s]?://\S+|www\.\S+", "", cleaned_text)
    
    # Remove empty lines or excessive whitespace
    cleaned_text = re.sub(r"\n\s*\n", "\n", cleaned_text)
    
    # Remove page numbers and unwanted parts
    cleaned_text = re.sub(r'\bPage \d+\b', '', cleaned_text)  # Remove page numbers
    cleaned_text = re.sub(r'\bCHAPTER\b.*', '', cleaned_text)  # Remove chapter titles 
    
    # Remove excess whitespace
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  
    
    # Extract only the relevant content starting from the first chapter
    start_index = cleaned_text.find("CHAPTER ONE")
    if start_index != -1:
        cleaned_text = cleaned_text[start_index:]
    
    return cleaned_text.strip()

# Clean the output: this function help to clean the question generated
def clean_output(output):
    # Strip leading and trailing whitespace
    output = output.strip()
    
    # Remove unnecessary leading dots
    if output.startswith('.'):
        output = output.lstrip('.')
    
    # Replace newlines with spaces
    output = output.replace('\n', ' ')
    
    # Remove any trailing periods that could be causing issues
    if output.endswith('.'):
        # Remove all trailing periods
        cleaned_text = output.rstrip('.')
        # If there was only one period, add it back
        if len(output) - len(cleaned_text) == 1:
            cleaned_text += '.'
    return output
# multi-question cleaning: pattern detection (numbered/bullet points) and question mark counting
def is_multi_question_combined(question):
    # Check for multiple questions using different patterns
    multi_question_pattern = r"(\d+\.\s|[-â€¢]\s)"
    multi_sentences = question.count('?') > 1
    numbered_matches = re.findall(multi_question_pattern, question)
    
    return len(numbered_matches) > 1 or multi_sentences
### remove not necessary quotes generated 
def strip_quotes(text):
    
    if isinstance(text, str):
        # Remove leading and trailing quotes
        if text.startswith('"') and text.endswith('"'):
            text = text[1:-1]  # Strip the first and last character (the quotes)
        elif text.startswith('"""') and text.endswith('"""'):
            text = text[3:-3]  # Strip triple quotes if they are present

        # Remove leading dash and following space
        if text.startswith('-'):
            text = text[2:]  # Strip the dash and the following space
        # Remove leading spaces
        text = text.lstrip('- ').lstrip()
    # Return the cleaned or original text
    return text

# Function to extract only the first question from numbered questions
def extract_first_question(text):
    if isinstance(text, str):
        # Find numbered questions using a regular expression
        match = re.match(r'^\d+\.\s*(.*?)(\d+\.|$)', text)
        if match:
            # Return the first question before any subsequent numbering
            return match.group(1).strip()
    return text  # Return the original text if no match
# Function to remove numbering from answers
def remove_numbering(text):
    if isinstance(text, str):
        # Remove numbered list (1., 2., 3., ...) and extra spaces
        text = re.sub(r'\d+\.\s*', '', text)
        # Remove extra spaces between sentences, if any
        text = re.sub(r'\s{2,}', ' ', text)
    return text
###############################################################################
# Function to save checkpoint
def save_checkpoint(questions, answers, chunks_done,checkpoint_file):
    df_checkpoint = pd.DataFrame({
        'Question': questions,
        'Answer': answers,
        'Chunk_Index': chunks_done
    })
    df_checkpoint.to_csv(checkpoint_file, index=False)

# Function to load checkpoint if it exists
def load_checkpoint(checkpoint_file):
    if os.path.exists(checkpoint_file):
        df_checkpoint = pd.read_csv(checkpoint_file)
        questions = df_checkpoint['Question'].tolist()
        answers = df_checkpoint['Answer'].tolist()
        chunks_done = df_checkpoint['Chunk_Index'].tolist()
        return questions, answers, chunks_done
    return [], [], []
###############################################################################
# Main Process
if __name__ == "__main__":
    choice = 'refining_qa'#refining_qa' gen_qa'
    
    # Set up your OpenAI API key
    openai.api_key = 'YOUR OPENAI API'
    # Set up the OpenAI LLM (make sure to set your OpenAI API key)
    llm = OpenAI(temperature=0.7, api_key=openai.api_key) 
    
    ###############################################################################
    # Test LLM connection
    try:
        # Example prompt
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is the capital of France?"}
            ],
            max_tokens=30  # Reduced to 30 to lower the token usage
        )
    
        # Print the output
        print(response.choices[0].message.content.strip())
        print("LLM Connection Successful!")
    except Exception as e:
        print("Failed to connect to OpenAI:", e)
        sys.exit()
    ###############################################################################
    
    if(choice=='gen_qa'):
        ###############################################################################

        ### document path
        path_txt = 'YOUT TEXT FILE'
        
        # Manually load the text
        with open(path_txt, "r",  encoding="utf-8", errors="ignore") as file:
            text_raw = file.read()
        ### Step 1 clening
        text_content = clean_text(text_raw)
        ### Step 2 Wrap the content in LangChain's document format
        documents = [Document(page_content=text_content)] ## 
        # Step 3: chunking process
        # text_splitter = RecursiveCharacterTextSplitter(
        #     chunk_size=300,
        #     chunk_overlap=20
        # )
        ### next step to improve answer question ######################################
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # Experiment with larger sizes
            chunk_overlap=50,  # Adjust overlap to retain more context
            separators=["\n\n", ". ", "! ", "? "]  # Split based on sentence boundaries
        )
        
        chunks = text_splitter.split_documents(documents)
        # Step 4: Define Prompts for Question and Answer Generation
        # question_prompt_template = PromptTemplate.from_template(
        #     "Generate a question based on the following text:\n\n{chunk}\n\nQuestion:"
        # )
        
        # answer_prompt_template = PromptTemplate.from_template(
        #     "Provide an answer based on the following text:\n\n{chunk}\n\nAnswer:"
        # )

        question_variants = [
            "Ask one direct and simple question about an important event in this text: {chunk}",
            "What is one single question someone might ask about this text? {chunk}",
            "Ask a single, straightforward question about a key object or action: {chunk}",
            "What one direct question can be asked about a specific detail in this text? {chunk}",
            "Formulate a single, direct question about the main idea of this passage: {chunk}",
            "Ask one focused question that highlights key information in this text: {chunk}",
            "Generate one direct question about a critical part of this text: {chunk}",
            "What one direct question would help understand the main ideas in this passage? {chunk}",
            "What single question can reveal important details in this text? {chunk}",
            "Form a direct question that focuses on the main themes of this passage: {chunk}",
            "What single question reveals a key aspect of this text? {chunk}",
            "Ask one direct question about the meaning of this section: {chunk}",
            "What single question highlights the key information in this text? {chunk}",
            "What one direct question explores the central themes of this passage? {chunk}",
            "Ask a single, focused question about the important elements of this text: {chunk}",
            "What single question would help understand the core idea of this passage? {chunk}"
        ]
        
        answer_variants = [
            "Provide one clear and concise answer based on this text: {chunk}",
            "What is the single most direct piece of information the text gives about this topic? {chunk}",
            "Summarize the key point of this text in one clear sentence: {chunk}",
            "What is the one direct inference that can be made from this passage? {chunk}",
            "Explain the main idea of this section in a single answer: {chunk}",
            "Describe the most important detail from this text in a single response: {chunk}",
            "Clarify the key element of this passage in one direct answer: {chunk}",
            "How would you explain the central idea in this text in a single answer? {chunk}",
            "Analyze the text and provide one direct answer: {chunk}",
            "What is the single most important insight from this passage? {chunk}",
            "Interpret the key point of this section with one direct answer: {chunk}",
            "Explain the most significant detail in this text with one direct answer: {chunk}",
            "Summarize the main concept of this passage in a single sentence: {chunk}",
            "What is the one main takeaway from this text? {chunk}",
            "Explain the most important information in this section in one answer: {chunk}",
            "Clarify the key detail of this text with one concise response: {chunk}"
        ]
        # Generate questions for each chunk using llm ################################
        # Initialize empty lists to store questions and answers
        questions = []
        answers = []
        chunks_done = []
        # Get the total number of chunks
        total_chunks = len(chunks)
        # Set the number of questions you want to generate per chunk
        num_iterations_per_chunk = 20
        # Save checkpoint every N chunks (e.g., every 5 chunks)
        save_checkpoint_every_n_chunks = 5
        # Iterate through each chunk
        # Checkpoint file path
        checkpoint_file = 'C:/Users/Enrico/Desktop/Progetti/20 TRANSFORMERS/chunk_checkpoint/questions_and_answers_checkpoint.csv'
        # Initialize questions and answers lists from the checkpoint, if it exists
        questions, answers, chunks_done = load_checkpoint(checkpoint_file)
        # Resume from the last saved chunk
        start_chunk = max(chunks_done) + 1 if chunks_done else 0
        #for i, chunk in enumerate(chunks):
        #for i, chunk in enumerate(chunks[start_chunk:], start=start_chunk):
        for i, chunk in enumerate(chunks[start_chunk:], start=start_chunk):
            chunk_text = chunk.page_content  # Get the content of the chunk
            print(f"Chunk {i+1}/{total_chunks}")
            for j in range(num_iterations_per_chunk):
                
                # Approach 1: Using llm.invoke
                attempts = 0
                valid_question = False
                
                while attempts < 2:  # Attempt twice
                    question_prompt = random.choice(question_variants).format(chunk=chunk_text)
                    question = llm.invoke(question_prompt)
                    question= clean_output(question)
                    
                    if '?' in question:  # Check if the question contains a '?'
                        valid_question = True
                        break  # If valid, break out of the retry loop
                    
                    attempts += 1
                    print(f"Attempt {attempts}: Invalid question generated using invoke")
                
                if not valid_question:
                    print("Skipping to next iteration after 2 failed attempts.")
                    continue  # Skip to the next iteration if both attempts failed
               
                ### some post processing ###############################################
                # Remove introductory phrases that are followed by a colon
                question =re.sub(r'^[^:]*:\s*', '', question).strip()
                
                if is_multi_question_combined(question): ## discard multi questions
                    continue
                #######################################################################
                # Choose a random answer variant
                answer_prompt = random.choice(answer_variants).format(chunk=chunk_text)
                answer = llm.invoke(answer_prompt)
                answer= clean_output(answer)
        
                ###  # Approach 2: Using OpenAI's chat.completions.create ###################
                attempts = 0
                valid_question2 = False
                while attempts < 2:  # Attempt twice
                   response = openai.chat.completions.create(
                       model="gpt-3.5-turbo",  # or "gpt-4"
                       messages=[
                           {"role": "system", "content": "You are an expert on 'Harry Potter and the Sorcerer's Stone.' Provide only **one single question** that is **directly related** to the provided text. Ensure that the question is **relevant and insightful**. Do not provide any additional text or explanation."},
                           {"role": "user", "content": question_prompt}
                       ],
                       temperature=0.6
                   )
                   question2 = response.choices[0].message.content
                   question2= clean_output(question2)
        
                   if '?' in question2:  # Check if the question contains a '?'
                       valid_question2 = True
                       break  # If valid, break out of the retry loop
        
                   attempts += 1
                   print(f"Attempt {attempts}: Invalid question generated using completions.create")
               
                if not valid_question2:
                   print("Skipping to next iteration after 2 failed attempts.")
                   continue  # Skip to the next iteration if both attempts failed
             
                ####### some post processing ###########################################
                # Remove introductory phrases that are followed by a colon --> bls bla bla: go go go 
                question2 = re.sub(r'^[^:]*:\s*', '', question2).strip()
                if is_multi_question_combined(question2): ## discard multi questions
                    continue
                ########################################################################
                # Choose a random answer variant
                #answer_prompt = random.choice(answer_variants).format(chunk=chunk_text)
                response = openai.chat.completions.create(
                    model="gpt-3.5-turbo",  # or "gpt-4"
                    messages=[
                        {"role": "system", "content": "You are an expert on 'Harry Potter and the Sorcerer's Stone.' Provide only **one single answer** based strictly on the text. Do not give a list, avoid multiple sentences, and limit your response to a **single, direct answer**."},
                        {"role": "user", "content": answer_prompt}
                    ],
                    temperature=0.6
                )
                answer2 = response.choices[0].message.content
                answer2= clean_output(answer2)
                #breakpoint()
                questions.append(question)
                answers.append(answer)
                questions.append(question2)
                answers.append(answer2)
        
                #breakpoint()
                print(f"Chunk {i+1} - Iteration {j+1}/{num_iterations_per_chunk}")
                # print(f"Question: {question}")
                # print(f"Answer: {answer}")
                # print("\n---\n")
                # Add the completed chunk index
                # Append 'i' twice to match the number of appends to questions and answers
                chunks_done.append(i)
                chunks_done.append(i)
            # Save the checkpoint every N chunks
            if (i + 1) % save_checkpoint_every_n_chunks == 0:
                 save_checkpoint(questions, answers, chunks_done,checkpoint_file)
                 print(f"Checkpoint saved after chunk {i+1}.")
            if (i==101):## repeat the forst 100 chunk
                break
        
        # Create a DataFrame with two columns: 'Question' and 'Answer'
        df_qa = pd.DataFrame({
            'Question': questions,
            'Answer': answers
        })
        
        
        # Apply the function to both the 'Question' and 'Answer' columns
        df_qa['Question'] = df_qa['Question'].apply(strip_quotes)
        df_qa['Answer'] = df_qa['Answer'].apply(strip_quotes)
        
        # Apply the function to the 'Question' column
        df_qa['Question'] = df_qa['Question'].apply(extract_first_question)
        # Apply the function to the 'Answer' column
        df_qa['Answer'] = df_qa['Answer'].apply(remove_numbering)
        
        final_file = './questions_and_answers_final.csv'
        df_qa.to_csv(final_file, index=False)
        print(f"Final qa csv saved .")
    ###########################################################################
    elif(choice=='refining_qa'):# THIS is a refinement (needs debug !)
        print('Refining step..')
        final_path = './chunk_checkpoint/'
        df_qa_row = pd.read_csv(final_path + 'questions_and_answers_final.csv')
        
        # Loop through each row of the DataFrame to refine the answers
        refined_answers = []
        total_count = len(df_qa_row)
        for index, row in df_qa_row.iterrows():
              question = row['Question']
              original_answer = row['Answer']
        
              # Create a refined answer based on the original question using OpenAI's API
              answer_prompt = f"{question}"
              response = openai.chat.completions.create(
                  model="gpt-3.5-turbo",  # or "gpt-4"
                  messages=[
                      {"role": "system", "content": "You are an expert on 'Harry Potter and the Sorcerer's Stone.' Provide only **one single answer** based strictly on the text. Do not give a list, avoid multiple sentences, and limit your response to a **single, direct answer**."},
                      {"role": "user", "content": answer_prompt}
                  ],
                  temperature=0.6
              )
              
              # Extract the refined answer from the API response
              refined_answer = response.choices[0].message.content
              refined_answer= clean_output(refined_answer)
              # Append the refined answer to the list
              refined_answers.append(refined_answer)
              # Print the remaining count
              remaining = total_count - (index + 1)
              print(f"Processed index {index}. Remaining: {remaining}. Total Count: {total_count}")
        
        # Add the refined answers to the DataFrame
        df_qa_row['refined_answer'] = refined_answers
        
          # Save the refined answers back to a new CSV file
        df_qa_row.to_csv(final_path+ 'questions_and_answers_refined.csv', index=False)
        
          # Print the final counts after refinement
        print(f'Refinement complete. Processed {total_count} questions.')
        print('Saved to questions_and_answers_refined.csv.')
        