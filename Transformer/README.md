# Transformer Model for Question-Answer Dataset from Harry Potter and the Sorcerer's Stone

This repository contains code for training and evaluating a Transformer model on a question-answer dataset derived from Harry Potter and the Sorcerer's Stone. The project demonstrates how to preprocess text data, train a Transformer model, and evaluate its performance.

## Overview
The project involves:
* Data Processing: Cleaning question-answer pairs.
* Model Training: Training a Transformer model on the processed data.
* Evaluation: Testing the trained model with sample questions and custom queries.

## Project Structure
	* train_and_eval.py: The main script for training and evaluating the Transformer model.
	* data_tokenized/: Directory to store tokenized data (question/answer)
	* checkpoints/: Directory for saving model checkpoints.



## Dependencies
Make sure you have the following packages installed:
* tensorflow numpy pandas scikit-learn sentencepiece contractions



## Data Processing: question/answer generation
* The dataset used consists of question-answer pairs from Harry Potter and the Sorcerer's Stone. For data processing details, refer to the **[Text_Processing](../Text_Processing)** folder's README 


## Usage
* Training
	-  To train the model, set the `choice` variable to `'train'` in the `train_and_eval.py` script:
		```python
		if __name__ == "__main__":
		    choice = 'train'
		    # (Training code here)
  		```
	- Ensure the path to your question-answer dataset is correctly specified in the path_aq variable. The dataset will be processed and saved as data_tokenized/data_token.pickle.
The training loop will save model weights to final_weights.h5 and periodically save checkpoints to ./checkpoints.

* Evaluation
	-  To evaluate the model, set the `choice` variable to `'eval'` in the `train_and_eval.py` script:
		```python
		if __name__ == "__main__":
		    choice = 'eval'
		    # (Evaluation code here)
  		```
	- The script will load the tokenized data and model weights, run evaluations, and print predictions for sample and custom questions.


## Example Output
- **Input:**  
  The original question provided to the model.

- **Predicted:**  
  The response generated by the model based on the input.

- **Actual:**  
  The correct or expected response for the given input.

Test Case 7410
* Input      : What dangerous reptiles did Dudley and Piers want to see at the reptile house, and what did Dudley find there?
* Predicted  : Dudley quickly found the largest snake in the place.
* Actual     : Dudley quickly found the largest snake in the place.
-----------------
Test Case 3168
* Input      : What significance does Hagrid's decision to gift Harry an owl as a birthday present hold in the narrative of 'Harry Potter and the Sorcerer's Stone'?
* Predicted  : Hagrid gives Harry an owl as a gift.
* Actual : Hagrid offers to get Harry an owl as a birthday present.
----------------------------------------

## Comments

In this project, I have demonstrated that the Transformer model is capable of generating meaningful and coherent responses even with a relatively small dataset. On random inputs, replies showed "traces of intelligence". However, it is important to note that the responses are not perfect.

The performance of the model can be significantly enhanced by incorporating additional data. Expanding the dataset to include text from other Harry Potter books, as well as supplementary information from external sources, will likely improve the model's accuracy and overall effectiveness.



## Authors

* Enrico Boscolo
