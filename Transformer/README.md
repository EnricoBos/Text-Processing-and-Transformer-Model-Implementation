# Transformer Model for Question-Answer Dataset from Harry Potter and the Sorcerer's Stone

This repository contains code for training and evaluating a Transformer model on a question-answer dataset derived from Harry Potter and the Sorcerer's Stone. The project demonstrates how to preprocess text data, train a Transformer model, and evaluate its performance.

## Overview
The project involves:
* Data Processing: Cleaning question-answer pairs.
* Model Training: Training a Transformer model on the processed data.
* Evaluation: Testing the trained model with sample questions and custom queries.

## Project Structure
	* train_and_eval.py: The main script for training and evaluating the Transformer model.
	* data_tokenized/: Directory to store tokenized data (question/answer)
	* checkpoints/: Directory for saving model checkpoints.



## Dependencies
Make sure you have the following packages installed:
* tensorflow numpy pandas scikit-learn sentencepiece contractions



## Data Processing: question/answer generation
* The dataset used consists of question-answer pairs from Harry Potter and the Sorcerer's Stone. For data processing details, refer to the **[Text_Processing](../Text_Processing)** folder's README 


## Usage
* Training
	-  To train the model, set the `choice` variable to `'train'` in the `train_and_eval.py` script:
		```python
		if __name__ == "__main__":
		    choice = 'train'
		    # (Training code here)
  		```
	- Ensure the path to your question-answer dataset is correctly specified in the path_aq variable. The dataset will be processed and saved as data_tokenized/data_token.pickle.
The training loop will save model weights to final_weights.h5 and periodically save checkpoints to ./checkpoints.

* Evaluation
	-  To evaluate the model, set the `choice` variable to `'eval'` in the `train_and_eval.py` script:
		```python
		if __name__ == "__main__":
		    choice = 'eval'
		    # (Evaluation code here)
  		```
	- The script will load the tokenized data and model weights, run evaluations, and print predictions for sample and custom questions.


## Example Output
- **Input:**  
  The original question provided to the model.

- **Predicted:**  
  The response generated by the model based on the input.

- **Actual:**  
  The correct or expected response for the given input.

-----------------
Test Case 7066
* Input      : What is the significance of the connection between Harry's scar hurting and Quirrell's fingers blistering in this scene?
* Predicted  : The connection between Harry's scar and Quirrell's fingers in this scene is significant because it is the connection between Harry and Voldemort, which is a connection between them.
* Actual : Quirrell's fingers blistering is a result of trying to touch Harry's skin, as a form of protection by his mother's love, causing pain to both of them.
----------------------------------------
Test Case Custom
* Input      : What is the name of Harry Potter's school?
* Predicted  : Hogwarts School of Witchcraft and Wizardry.
* Actual     : Hogwarts School of Witchcraft and Wizardry.
----------------------------------------
Test Case Custom
* Input      : What is the significance of the Mirror of Erised, and what does it show Harry when he looks into it?
* Predicted  : The Mirror of Erised shows Harry his deepest desire, which is to see his deceased parents.
* Actual     : The Mirror of Erised shows the deepest desires of a person's heart. When Harry looks into the mirror, he sees his parents, who died when he was a baby.

## Comments

In this project, I have demonstrated that the Transformer model is capable of generating meaningful and coherent responses even with a relatively small dataset. On random inputs, replies showed "traces of intelligence". However, it is important to note that the responses are not perfect.

The performance of the model can be significantly enhanced by incorporating additional data. Expanding the dataset to include text from other Harry Potter books, as well as supplementary information from external sources, will likely improve the model's accuracy and overall effectiveness.



## Authors

* Enrico Boscolo
